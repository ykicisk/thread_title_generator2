{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train_rnn_generator\n",
    "\n",
    "RNNでスレタイ生成をするGeneratorを学習する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.text_encoder import NanJThreadTitleEncoder\n",
    "text_encoder = NanJThreadTitleEncoder.load_from_file(\"../model/text_encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../model/input.pickle\", \"rb\") as f:\n",
    "    ids = pickle.load(f)\n",
    "input_tensor = tf.keras.preprocessing.sequence.pad_sequences(ids, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1452049 161339\n"
     ]
    }
   ],
   "source": [
    "input_tensor_train, input_tensor_valid = train_test_split(input_tensor, test_size=0.1)\n",
    "print(len(input_tensor_train), len(input_tensor_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices(input_tensor_train).shuffle(BUFFER_SIZE)\n",
    "BUFFER_SIZE = len(input_tensor_valid)\n",
    "dataset_valid = tf.data.Dataset.from_tensor_slices(input_tensor_valid).shuffle(BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "vocab_size = text_encoder.vocab_size()\n",
    "embedding_dim = 128\n",
    "gen_units = 128\n",
    "num_stacks = 4\n",
    "seq_len = input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model import create_rnn_generator_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = create_rnn_generator_model(gen_units, embedding_dim, vocab_size, num_stacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1, 128)       2560512     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gru (GRU)                       [(None, 1, 128), (No 99072       embedding[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     [(None, 1, 128), (No 99072       gru[0][0]                        \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gru_2 (GRU)                     [(None, 1, 128), (No 99072       gru_1[0][0]                      \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gru_3 (GRU)                     [(None, 128), (None, 99072       gru_2[0][0]                      \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 20004)        2580516     gru_3[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 5,537,316\n",
      "Trainable params: 5,537,316\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(num_generations):\n",
    "    \"\"\"\n",
    "    Generatorを使ってnum_generations個の文書（トークンID列）を生成する\n",
    "    \"\"\"\n",
    "    gen_states = []\n",
    "    for _ in range(num_stacks):\n",
    "        gen_states.append(tf.zeros((num_generations, gru_units)))\n",
    "    \n",
    "    gen_input = tf.expand_dims([text_encoder.SOS_TOKEN_ID] * num_generations, 1)\n",
    "    generation_ids_list = [gen_input]\n",
    "    for i in range(seq_len - 1):\n",
    "        gen_input = generation_ids_list[-1]\n",
    "        gen_output = generator([gen_input] + gen_states)\n",
    "        predictions = gen_output[0]\n",
    "        gen_states = gen_output[1:]\n",
    "        next_ids = tf.random.categorical(predictions, num_samples=1, dtype=\"int32\")\n",
    "        generation_ids_list.append(next_ids)\n",
    "    generation_ids = tf.concat(generation_ids_list, axis=1)\n",
    "    return generation_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初期状態\n",
    "generation_ids = generate(10).numpy()\n",
    "for ids in generation_ids:\n",
    "    print(\"Generation:\", \" \".join(text_encoder.decode(ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp):    # inp: (BATCH_SIZE, seq_len)\n",
    "    loss = 0\n",
    "    \n",
    "    gen_states = []\n",
    "    for _ in range(num_stacks):\n",
    "        gen_states.append(tf.zeros((BATCH_SIZE, gen_units)))\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        for i in range(inp.shape[1] - 1):\n",
    "            gen_input = tf.expand_dims(inp[:, i], 1)\n",
    "            gen_output = generator([gen_input] + gen_states)\n",
    "            predictions = gen_output[0]\n",
    "            gen_states = gen_output[1:]\n",
    "            loss += loss_object(inp[:, i+1], predictions)\n",
    "\n",
    "    batch_loss = loss / BATCH_SIZE\n",
    "    \n",
    "    gradients = tape.gradient(loss, generator.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, generator.trainable_variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def valid_step(inp):\n",
    "    loss = 0\n",
    "    \n",
    "    gen_states = []\n",
    "    for _ in range(num_stacks):\n",
    "        gen_states.append(tf.zeros((BATCH_SIZE, gen_units)))\n",
    "    \n",
    "    for i in range(inp.shape[1] - 1):\n",
    "        gen_input = tf.expand_dims(inp[:, i], 1)\n",
    "        gen_output = generator([gen_input] + gen_states)\n",
    "        predictions = gen_output[0]\n",
    "        gen_states = gen_output[1:]\n",
    "        loss += loss_object(inp[:, i+1], predictions)\n",
    "\n",
    "    batch_loss = loss / BATCH_SIZE\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "steps_per_epoch_train = len(input_tensor_train)//BATCH_SIZE\n",
    "steps_per_epoch_valid = len(input_tensor_valid)//BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset_train.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset_valid = dataset_valid.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../model/rnn_generator\"):\n",
    "    os.mkdir(\"../model/rnn_generator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    # TRAIN\n",
    "    total_train_loss = 0\n",
    "    for (batch, inp) in enumerate(dataset_train.take(steps_per_epoch_train)):\n",
    "        # batch_start = time.time()\n",
    "        batch_loss = train_step(inp)\n",
    "        total_train_loss += batch_loss\n",
    "        # print('Time taken for 1 batch {} sec'.format(time.time() - batch_start))\n",
    "\n",
    "        if batch % 500 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "    generator.save_weights(f\"../model/rnn_generator/weights_epoch{epoch+1}.h5\")\n",
    "\n",
    "    print('Train Epoch {} Loss {:.4f}'.format(epoch + 1, total_train_loss / steps_per_epoch_train))\n",
    "    \n",
    "    # VALIDATION\n",
    "    total_valid_loss = 0\n",
    "    for (batch, inp) in enumerate(dataset_valid.take(steps_per_epoch_valid)):\n",
    "        batch_loss = valid_step(inp)\n",
    "        total_valid_loss += batch_loss\n",
    "        \n",
    "    print('Validation Loss {:.4f}'.format(total_valid_loss / steps_per_epoch_valid))\n",
    "    \n",
    "    # GENERATION\n",
    "    generation_ids = generate(10).numpy()\n",
    "    for ids in generation_ids:\n",
    "        print(\"Generation:\", \" \".join(text_encoder.decode(ids)))\n",
    "    \n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
